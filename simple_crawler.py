#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆå¤šå¹³å°çˆ¬è™«å·¥å…·
æ”¯æŒï¼šå°çº¢ä¹¦ã€æŠ–éŸ³ã€å¾®åšã€çŸ¥ä¹ç­‰å¹³å°
ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨
"""

import asyncio
import json
import os
import time
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin

import aiofiles
from playwright.async_api import async_playwright, Browser, BrowserContext, Page


class Config:
    """é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.platform = "xhs"  # å¹³å°ï¼šxhs, dy, wb, zhihu
        self.keywords = "ç¼–ç¨‹,æŠ€æœ¯"  # æœç´¢å…³é”®è¯
        self.login_type = "qrcode"  # ç™»å½•æ–¹å¼ï¼šqrcode, cookie
        self.cookies = ""  # Cookieå­—ç¬¦ä¸²
        
        # çˆ¬å–é…ç½®
        self.max_items = 50  # æœ€å¤§çˆ¬å–æ•°é‡
        self.enable_comments = True  # æ˜¯å¦çˆ¬å–è¯„è®º
        self.max_comments = 10  # æ¯ä¸ªå¸–å­æœ€å¤§è¯„è®ºæ•°
        self.enable_images = False  # æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        
        # æµè§ˆå™¨é…ç½®
        self.headless = False  # æ— å¤´æ¨¡å¼
        self.save_login_state = True  # ä¿å­˜ç™»å½•çŠ¶æ€
        self.user_data_dir = "./browser_data"  # æµè§ˆå™¨æ•°æ®ç›®å½•
        
        # æ•°æ®å­˜å‚¨é…ç½®
        self.save_format = "json"  # å­˜å‚¨æ ¼å¼ï¼šjson, csv, sqlite
        self.output_dir = "./data"  # è¾“å‡ºç›®å½•
        
        # ä»£ç†é…ç½®
        self.enable_proxy = False
        self.proxy_server = ""
        
        # å»¶è¿Ÿé…ç½®
        self.delay_between_requests = 1  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        self.delay_between_pages = 2  # é¡µé¢é—´éš”ï¼ˆç§’ï¼‰


class DataStore:
    """æ•°æ®å­˜å‚¨ç±»"""
    
    def __init__(self, config: Config):
        self.config = config
        self.output_dir = config.output_dir
        self.data = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
    
    async def save_item(self, item: Dict):
        """ä¿å­˜å•ä¸ªæ•°æ®é¡¹"""
        self.data.append(item)
    
    async def save_to_file(self):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not self.data:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        platform = self.config.platform
        
        if self.config.save_format == "json":
            filename = f"{platform}_{timestamp}.json"
            filepath = os.path.join(self.output_dir, filename)
            
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(self.data, ensure_ascii=False, indent=2))
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            print(f"ğŸ“Š å…±ä¿å­˜ {len(self.data)} æ¡æ•°æ®")
        
        elif self.config.save_format == "csv":
            import csv
            filename = f"{platform}_{timestamp}.csv"
            filepath = os.path.join(self.output_dir, filename)
            
            if self.data:
                fieldnames = self.data[0].keys()
                with open(filepath, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(self.data)
                
                print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                print(f"ğŸ“Š å…±ä¿å­˜ {len(self.data)} æ¡æ•°æ®")


class BaseCrawler(ABC):
    """çˆ¬è™«åŸºç±»"""
    
    def __init__(self, config: Config, store: DataStore):
        self.config = config
        self.store = store
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
    
    async def start(self):
        """å¯åŠ¨çˆ¬è™«"""
        try:
            print(f"ğŸš€ å¼€å§‹çˆ¬å– {self.config.platform} å¹³å°...")
            
            # å¯åŠ¨æµè§ˆå™¨
            await self.launch_browser()
            
            # ç™»å½•
            await self.login()
            
            # æ‰§è¡Œçˆ¬å–
            await self.crawl()
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        finally:
            await self.cleanup()
    
    async def launch_browser(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        playwright = await async_playwright().start()
        
        # æµè§ˆå™¨å¯åŠ¨å‚æ•°
        browser_args = [
            '--no-sandbox',
            '--disable-blink-features=AutomationControlled',
            '--disable-dev-shm-usage'
        ]
        
        # ä»£ç†é…ç½®
        proxy = None
        if self.config.enable_proxy and self.config.proxy_server:
            proxy = {"server": self.config.proxy_server}
        
        # å¯åŠ¨æµè§ˆå™¨
        self.browser = await playwright.chromium.launch(
            headless=self.config.headless,
            args=browser_args,
            proxy=proxy
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context_options = {
            "viewport": {"width": 1920, "height": 1080},
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        if self.config.save_login_state:
            context_options["user_data_dir"] = os.path.join(
                self.config.user_data_dir, 
                self.config.platform
            )
        
        self.context = await self.browser.new_context(**context_options)
        self.page = await self.context.new_page()
        
        print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
    
    @abstractmethod
    async def login(self):
        """ç™»å½•æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°"""
        pass
    
    @abstractmethod
    async def crawl(self):
        """çˆ¬å–æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°"""
        pass
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.page:
            await self.page.close()
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        
        # ä¿å­˜æ•°æ®
        await self.store.save_to_file()
        print("âœ… çˆ¬å–å®Œæˆ")


class XiaoHongShuCrawler(BaseCrawler):
    """å°çº¢ä¹¦çˆ¬è™«"""
    
    async def login(self):
        """å°çº¢ä¹¦ç™»å½•"""
        print("ğŸ” æ­£åœ¨ç™»å½•å°çº¢ä¹¦...")
        
        await self.page.goto("https://www.xiaohongshu.com")
        await self.page.wait_for_load_state("networkidle")
        
        if self.config.login_type == "qrcode":
            # ç­‰å¾…ç”¨æˆ·æ‰«ç ç™»å½•
            print("ğŸ“± è¯·ä½¿ç”¨å°çº¢ä¹¦APPæ‰«æäºŒç»´ç ç™»å½•...")
            
            # ç­‰å¾…ç™»å½•æˆåŠŸ
            try:
                await self.page.wait_for_selector('[data-testid="user-avatar"]', timeout=300000)
                print("âœ… ç™»å½•æˆåŠŸ")
            except Exception as e:
                print("âŒ ç™»å½•è¶…æ—¶æˆ–å¤±è´¥")
                raise e
    
    async def crawl(self):
        """çˆ¬å–å°çº¢ä¹¦å†…å®¹"""
        print("ğŸ” å¼€å§‹æœç´¢å…³é”®è¯...")
        
        for keyword in self.config.keywords.split(','):
            keyword = keyword.strip()
            if not keyword:
                continue
            
            print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
            
            # æœç´¢é¡µé¢
            search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
            await self.page.goto(search_url)
            await self.page.wait_for_load_state("networkidle")
            
            # çˆ¬å–æœç´¢ç»“æœ
            await self.crawl_search_results(keyword)
            
            # é¡µé¢é—´éš”
            await asyncio.sleep(self.config.delay_between_pages)
    
    async def crawl_search_results(self, keyword: str):
        """çˆ¬å–æœç´¢ç»“æœ"""
        items_count = 0
        
        while items_count < self.config.max_items:
            # è·å–ç¬”è®°åˆ—è¡¨
            notes = await self.page.query_selector_all('[data-testid="note-item"]')
            
            for note in notes:
                if items_count >= self.config.max_items:
                    break
                
                try:
                    # æå–ç¬”è®°ä¿¡æ¯
                    note_data = await self.extract_note_data(note, keyword)
                    if note_data:
                        await self.store.save_item(note_data)
                        items_count += 1
                        print(f"ğŸ“ å·²çˆ¬å– {items_count} æ¡ç¬”è®°")
                    
                    await asyncio.sleep(self.config.delay_between_requests)
                    
                except Exception as e:
                    print(f"âš ï¸ æå–ç¬”è®°æ•°æ®å¤±è´¥: {e}")
                    continue
            
            # å°è¯•ç¿»é¡µ
            try:
                next_button = await self.page.query_selector('[data-testid="next-page"]')
                if next_button:
                    await next_button.click()
                    await self.page.wait_for_load_state("networkidle")
                else:
                    break
            except:
                break
    
    async def extract_note_data(self, note_element, keyword: str) -> Optional[Dict]:
        """æå–ç¬”è®°æ•°æ®"""
        try:
            # æå–æ ‡é¢˜
            title_element = await note_element.query_selector('[data-testid="note-title"]')
            title = await title_element.text_content() if title_element else ""
            
            # æå–é“¾æ¥
            link_element = await note_element.query_selector('a')
            link = await link_element.get_attribute('href') if link_element else ""
            if link and not link.startswith('http'):
                link = urljoin("https://www.xiaohongshu.com", link)
            
            # æå–ä½œè€…
            author_element = await note_element.query_selector('[data-testid="author-name"]')
            author = await author_element.text_content() if author_element else ""
            
            # æå–ç‚¹èµæ•°
            like_element = await note_element.query_selector('[data-testid="like-count"]')
            likes = await like_element.text_content() if like_element else "0"
            
            return {
                "platform": "xiaohongshu",
                "keyword": keyword,
                "title": title.strip() if title else "",
                "author": author.strip() if author else "",
                "link": link,
                "likes": likes.strip() if likes else "0",
                "crawl_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âš ï¸ æå–ç¬”è®°æ•°æ®å¤±è´¥: {e}")
            return None


class DouYinCrawler(BaseCrawler):
    """æŠ–éŸ³çˆ¬è™«"""
    
    async def login(self):
        """æŠ–éŸ³ç™»å½•"""
        print("ğŸ” æ­£åœ¨ç™»å½•æŠ–éŸ³...")
        
        await self.page.goto("https://www.douyin.com")
        await self.page.wait_for_load_state("networkidle")
        
        if self.config.login_type == "qrcode":
            print("ğŸ“± è¯·ä½¿ç”¨æŠ–éŸ³APPæ‰«æäºŒç»´ç ç™»å½•...")
            
            try:
                await self.page.wait_for_selector('.avatar', timeout=300000)
                print("âœ… ç™»å½•æˆåŠŸ")
            except Exception as e:
                print("âŒ ç™»å½•è¶…æ—¶æˆ–å¤±è´¥")
                raise e
    
    async def crawl(self):
        """çˆ¬å–æŠ–éŸ³å†…å®¹"""
        print("ğŸ” å¼€å§‹æœç´¢å…³é”®è¯...")
        
        for keyword in self.config.keywords.split(','):
            keyword = keyword.strip()
            if not keyword:
                continue
            
            print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
            
            # æœç´¢é¡µé¢
            search_url = f"https://www.douyin.com/search/{keyword}"
            await self.page.goto(search_url)
            await self.page.wait_for_load_state("networkidle")
            
            # çˆ¬å–æœç´¢ç»“æœ
            await self.crawl_search_results(keyword)
            
            await asyncio.sleep(self.config.delay_between_pages)
    
    async def crawl_search_results(self, keyword: str):
        """çˆ¬å–æœç´¢ç»“æœ"""
        items_count = 0
        
        while items_count < self.config.max_items:
            # è·å–è§†é¢‘åˆ—è¡¨
            videos = await self.page.query_selector_all('[data-e2e="scroll-item"]')
            
            for video in videos:
                if items_count >= self.config.max_items:
                    break
                
                try:
                    video_data = await self.extract_video_data(video, keyword)
                    if video_data:
                        await self.store.save_item(video_data)
                        items_count += 1
                        print(f"ğŸ¬ å·²çˆ¬å– {items_count} ä¸ªè§†é¢‘")
                    
                    await asyncio.sleep(self.config.delay_between_requests)
                    
                except Exception as e:
                    print(f"âš ï¸ æå–è§†é¢‘æ•°æ®å¤±è´¥: {e}")
                    continue
            
            # æ»šåŠ¨åŠ è½½æ›´å¤š
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.page.wait_for_timeout(2000)
    
    async def extract_video_data(self, video_element, keyword: str) -> Optional[Dict]:
        """æå–è§†é¢‘æ•°æ®"""
        try:
            # æå–æ ‡é¢˜
            title_element = await video_element.query_selector('[data-e2e="video-title"]')
            title = await title_element.text_content() if title_element else ""
            
            # æå–ä½œè€…
            author_element = await video_element.query_selector('[data-e2e="video-author"]')
            author = await author_element.text_content() if author_element else ""
            
            # æå–ç‚¹èµæ•°
            like_element = await video_element.query_selector('[data-e2e="like-count"]')
            likes = await like_element.text_content() if like_element else "0"
            
            # æå–é“¾æ¥
            link_element = await video_element.query_selector('a')
            link = await link_element.get_attribute('href') if link_element else ""
            if link and not link.startswith('http'):
                link = urljoin("https://www.douyin.com", link)
            
            return {
                "platform": "douyin",
                "keyword": keyword,
                "title": title.strip() if title else "",
                "author": author.strip() if author else "",
                "link": link,
                "likes": likes.strip() if likes else "0",
                "crawl_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âš ï¸ æå–è§†é¢‘æ•°æ®å¤±è´¥: {e}")
            return None


class WeiboCrawler(BaseCrawler):
    """å¾®åšçˆ¬è™«"""
    
    async def login(self):
        """å¾®åšç™»å½•"""
        print("ğŸ” æ­£åœ¨ç™»å½•å¾®åš...")
        
        await self.page.goto("https://weibo.com")
        await self.page.wait_for_load_state("networkidle")
        
        if self.config.login_type == "qrcode":
            print("ğŸ“± è¯·ä½¿ç”¨å¾®åšAPPæ‰«æäºŒç»´ç ç™»å½•...")
            
            try:
                await self.page.wait_for_selector('.avatar', timeout=300000)
                print("âœ… ç™»å½•æˆåŠŸ")
            except Exception as e:
                print("âŒ ç™»å½•è¶…æ—¶æˆ–å¤±è´¥")
                raise e
    
    async def crawl(self):
        """çˆ¬å–å¾®åšå†…å®¹"""
        print("ğŸ” å¼€å§‹æœç´¢å…³é”®è¯...")
        
        for keyword in self.config.keywords.split(','):
            keyword = keyword.strip()
            if not keyword:
                continue
            
            print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
            
            # æœç´¢é¡µé¢
            search_url = f"https://s.weibo.com/weibo?q={keyword}"
            await self.page.goto(search_url)
            await self.page.wait_for_load_state("networkidle")
            
            # çˆ¬å–æœç´¢ç»“æœ
            await self.crawl_search_results(keyword)
            
            await asyncio.sleep(self.config.delay_between_pages)
    
    async def crawl_search_results(self, keyword: str):
        """çˆ¬å–æœç´¢ç»“æœ"""
        items_count = 0
        
        while items_count < self.config.max_items:
            # è·å–å¾®åšåˆ—è¡¨
            weibos = await self.page.query_selector_all('.card-wrap')
            
            for weibo in weibos:
                if items_count >= self.config.max_items:
                    break
                
                try:
                    weibo_data = await self.extract_weibo_data(weibo, keyword)
                    if weibo_data:
                        await self.store.save_item(weibo_data)
                        items_count += 1
                        print(f"ğŸ“ å·²çˆ¬å– {items_count} æ¡å¾®åš")
                    
                    await asyncio.sleep(self.config.delay_between_requests)
                    
                except Exception as e:
                    print(f"âš ï¸ æå–å¾®åšæ•°æ®å¤±è´¥: {e}")
                    continue
            
            # ç¿»é¡µ
            try:
                next_button = await self.page.query_selector('.next')
                if next_button:
                    await next_button.click()
                    await self.page.wait_for_load_state("networkidle")
                else:
                    break
            except:
                break
    
    async def extract_weibo_data(self, weibo_element, keyword: str) -> Optional[Dict]:
        """æå–å¾®åšæ•°æ®"""
        try:
            # æå–å†…å®¹
            content_element = await weibo_element.query_selector('.txt')
            content = await content_element.text_content() if content_element else ""
            
            # æå–ä½œè€…
            author_element = await weibo_element.query_selector('.name')
            author = await author_element.text_content() if author_element else ""
            
            # æå–æ—¶é—´
            time_element = await weibo_element.query_selector('.from')
            time_str = await time_element.text_content() if time_element else ""
            
            # æå–è½¬å‘æ•°
            forward_element = await weibo_element.query_selector('.card-act li:nth-child(2)')
            forwards = await forward_element.text_content() if forward_element else "0"
            
            # æå–è¯„è®ºæ•°
            comment_element = await weibo_element.query_selector('.card-act li:nth-child(3)')
            comments = await comment_element.text_content() if comment_element else "0"
            
            # æå–ç‚¹èµæ•°
            like_element = await weibo_element.query_selector('.card-act li:nth-child(4)')
            likes = await like_element.text_content() if like_element else "0"
            
            return {
                "platform": "weibo",
                "keyword": keyword,
                "content": content.strip() if content else "",
                "author": author.strip() if author else "",
                "time": time_str.strip() if time_str else "",
                "forwards": forwards.strip() if forwards else "0",
                "comments": comments.strip() if comments else "0",
                "likes": likes.strip() if likes else "0",
                "crawl_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âš ï¸ æå–å¾®åšæ•°æ®å¤±è´¥: {e}")
            return None


class CrawlerFactory:
    """çˆ¬è™«å·¥å‚ç±»"""
    
    CRAWLERS = {
        "xhs": XiaoHongShuCrawler,
        "dy": DouYinCrawler,
        "wb": WeiboCrawler,
    }
    
    @staticmethod
    def create_crawler(platform: str, config: Config, store: DataStore):
        """åˆ›å»ºçˆ¬è™«å®ä¾‹"""
        crawler_class = CrawlerFactory.CRAWLERS.get(platform)
        if not crawler_class:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")
        return crawler_class(config, store)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç®€åŒ–ç‰ˆå¤šå¹³å°çˆ¬è™«å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # åˆ›å»ºæ•°æ®å­˜å‚¨
    store = DataStore(config)
    
    # åˆ›å»ºçˆ¬è™«
    try:
        crawler = CrawlerFactory.create_crawler(config.platform, config, store)
        await crawler.start()
    except Exception as e:
        print(f"âŒ çˆ¬è™«å¯åŠ¨å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main()) 